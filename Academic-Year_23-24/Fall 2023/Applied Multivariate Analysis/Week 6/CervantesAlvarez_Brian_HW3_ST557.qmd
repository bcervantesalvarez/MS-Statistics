---
title: "ST551: Homework 0"
author: 
    - "Brian Cervantes Alvarez"
date: "11-13-2023"
format: PrettyPDF-pdf
execute: 
  warning: false
  message: false
editor: visual
---

## Question 1

### Part A

No, we reject the null hypothesis that the average temperature in Corvallis has stayed the same. It is highly suggested to go with the alternative where the average temperature has changed over the span of 50 years or 5 decades.

```{r}

library(MASS)
# Read the data
tempData <- read.csv("TempData.csv")

# Extract the relevant columns
decadesData <- tempData[, c('X1950s', 'X1960s', 'X1970s', 'X1980s', 'X1990s', 'X2000s')]

# Calculate mean vector and covariance matrix
meanVec <- colMeans(decadesData)
covMat <- cov(decadesData)

n <- nrow(decadesData)
p <- ncol(decadesData)

# Calculate Hotelling's T2 statistic
T2 <- n * t(meanVec) %*% solve(covMat) %*% meanVec

# Degrees of freedom
df1 <- p
df2 <- n - p

# Calculate the p-value using F-distribution
pval <- pf(T2, df1, df2, lower.tail = FALSE)

# Results
print(paste0("Hotelling's T2 statistic: ", T2))
print(paste0("Degrees of freedom: ", df1, ", ", df2))
print(paste0("P-value: ", pval))

# Check if the p-value is less than the significance level (e.g., 0.05)
if (pval < 0.05) {
  print("Reject the null hypothesis. Not all means are equal.")
} else {
  print("Fail to reject the null hypothesis. Means are equal.")
}
```
{{< pagebreak >}}

## Question 1

### Part B

```{r}
alpha <- 0.05
# Number of comparisons
m <- 5  
bonferroniFactor <- qt(1 - alpha / (2 * m), df = df2)

# Confidence Intervals
confIntervals <- matrix(NA, nrow = m, ncol = 2)
for (i in 2:(m + 1)) {
  diffMean <- meanVec[i] - meanVec[1]
  se <- sqrt(covMat[i, i] / n + covMat[1, 1] / n - 2 * covMat[i, 1] / n)
  marginOfError <- bonferroniFactor * se
  confIntervals[i - 1, ] <- c(diffMean - marginOfError, diffMean + marginOfError)
}

# Display Confidence Intervals
for (i in 2:(m + 1)) {
  print(paste0("95% Bonferroni CI for (mu", i, " - mu1): [", confIntervals[i - 1, 1], ", ", confIntervals[i - 1, 2], "]"))
}

# Check if intervals include 0
zeroIncluded <- any(confIntervals[, 1] <= 0 & confIntervals[, 2] >= 0)

# Conclusion
if (zeroIncluded) {
  print("At least one interval includes 0. Conclude that the corresponding means are not significantly different.")
} else {
  print("None of the intervals include 0. Conclude that all corresponding means are significantly different.")
}
```

{{< pagebreak >}}

## Question 2

### Part A


Upon visual inspection, there are variations in the values across the matrices, indicating changes in the relationships between variables over time.


```{r}
skullData <- read.csv("SkullData.csv")

names(skullData)

# Split the data by Year
splitData <- split(skullData[, 2:5], skullData[, 1])

# Compute covariance matrices for each Year
covMatrices <- lapply(splitData, cov)
covMatrices
```

{{< pagebreak >}}

## Question 2

### Part B

The small p-value suggests notable variations in skull sizes across different years. While the MANOVA test strongly supports rejecting the null hypothesis in favor of the alternative, the researcher now needs to provide a coherent narrative explaining why interbreeding is considered the primary factor causing these differences in skull sizes.


```{r}
# Fit the MANOVA model
manovaModel <- manova(cbind(MB, BH, BL, NH) ~ Year, data = skullData)
# Fit the MANOVA model using Wilks' Lambda
summary(manovaModel, test = "Wilks")
```


{{< pagebreak >}}

## Question 2

### Part C

For each skull measurement (MB, BH, BL, NH), the univariate tests indicate that the "Year" variable has a statistically significant effect, signifying variations in these skull features over time. These findings provide statistical support for the idea that skull measurements change across different years.

If you reject $H_0: {\mu}_1 = {\mu}_2 = {\mu}_3 = {\mu}_4 = {\mu}_5$ whenever any of the univariate ANOVAs are significant at level $\alpha$, the overall probability of a Type I error is not guaranteed to be controlled at level $\alpha$. This is because the Bonferroni correction ($\alpha$) is a conservative adjustment. The actual overall significance level might be lower than $\alpha$, which reduces the chance of a Type I error but also reduces the power of the test!

In this case, all $H_0: {\mu}_1 = {\mu}_2 = {\mu}_3 = {\mu}_4 = {\mu}_5$ are $\le \alpha$, hence the overall probability of a Type 1 error is controlled at level $\alpha$



```{r}
# Univariate ANOVA for each variable
mbAnova <- aov(MB ~ Year, data = skullData)
bhAnova <- aov(BH ~ Year, data = skullData)
blAnova <- aov(BL ~ Year, data = skullData)
nhAnova <- aov(NH ~ Year, data = skullData)

# Check the p-values for each univariate ANOVA => Pr(>F)
summary(mbAnova)
summary(bhAnova)
summary(blAnova)
summary(nhAnova)
```

{{< pagebreak >}}

## Question 3

### Part A

```{r}
# Load the data
pollutionData <- read.csv("PollutionData.csv")

# Fit the multivariate multiple regression model
model <- lm(cbind(NO2, O3) ~ Wind + SolarRad, data = pollutionData)

```


{{< pagebreak >}}

## Question 3

### Part B

```{r}

```


{{< pagebreak >}}

## Question 3

### Part C

```{r}

```


{{< pagebreak >}}

## Question 4

### Part A


It is recommended to use the correlation matrix rather than the covariance matrix. The reason for this is that PCA is sensitive to the scale of the variables, and using the correlation matrix standardizes the variables to have mean 0 and standard deviation 1.

```{r}
trackData <- read.csv("TrackData.csv")

# Extract the columns containing distances
distanceColumns <- c("X100m.s", "X200m.s", "X400m.s", "X800m.m", "X1500m.m", "X5000m.m", "X10000m.m", "Marathon.m")
distanceData <- trackData[, c("Country", distanceColumns)]

# Covariance matrix S
covMatrix <- cov(distanceData[, -1])
covMatrix

# Correlation matrix R
corMatrix <- cor(distanceData[, -1])
corMatrix
```


{{< pagebreak >}}

## Question 4

### Part B

```{r}
eigenS <- eigen(covMatrix)
eigenS
```


{{< pagebreak >}}

## Question 4

### Part C

```{r}
eigenR <- eigen(corMatrix)
eigenR
```


{{< pagebreak >}}

## Question 4

### Part D

```{r}
# PCA using covariance matrix
pcaS <- eigenS

# PCA using correlation matrix
pcaR <- eigenR

# Plots
par(mfrow = c(2, 2))  # Setting up a 2x2 grid for subplots

# Plotting loadings for the first four principal components of S
for (i in 1:4) {
  plot(1:(ncol(distanceData) - 1), pcaS$vectors[, i], 
       main = paste("PC", i, "Loadings (S)"))
}

# Plotting loadings for the first four principal components of R
for (i in 1:4) {
  plot(1:(ncol(distanceData) - 1), pcaR$vectors[, i], 
       main = paste("PC", i, "Loadings (R)"))
}

```


{{< pagebreak >}}

## Question 4

### Part E

The loadings for PCA 1 for Matrix S shows a strong, positive influence on the principal component.

{{< pagebreak >}}

## Question 4

### Part F

So, if the first principal component's loadings form a parabola for the distance records, it means that the way these races impact the data is not just a simple straight line. There could be some tricky and not-so-straight patterns going on. This suggests we should take a closer look at how performances in these races affect the overall patterns in our data, especially when it comes to the first principal component.


{{< pagebreak >}}

## Question 4

### Part G

The straight line pattern in the loadings of the second principal component helps us understand how various distance records affect the overall trends we see in the data. This line gives us a clearer picture of how each type of race contributes to the patterns we're studying. It lets us look more closely at how performances differ across these distances from the dataset.


{{< pagebreak >}}

## Question 4

### Part H

```{r}
# Calculate cumulative variance explained
cumVarianceS <- cumsum(pcaS$values) / sum(pcaS$values)
cumVarianceR <- cumsum(pcaR$values) / sum(pcaR$values)

# Scree plot for S
plot(1:length(pcaS$values), pcaS$values, type = 'b', 
     main = "Scree Plot (S)", xlab = "Principal Component", ylab = "Eigenvalue")

# Cumulative variance explained plot for S
plot(1:length(pcaS$values), cumVarianceS, type = 'b', 
     main = "Cumulative Variance Explained (S)", xlab = "Principal Component", ylab = "Cumulative Variance Explained")

# Scree plot for R
plot(1:length(pcaR$values), pcaR$values, type = 'b', 
     main = "Scree Plot (R)", xlab = "Principal Component", ylab = "Eigenvalue")

# Cumulative variance explained plot for R
plot(1:length(pcaR$values), cumVarianceR, type = 'b', 
     main = "Cumulative Variance Explained (R)", xlab = "Principal Component", ylab = "Cumulative Variance Explained")
```


{{< pagebreak >}}

## Question 4

### Part I

Checking out the scree plot, I'd stick with the first 3 components because that's where the variance stops increasing much. But, when I consider the total variance, I usually stop at the point where the components add up to around 80%. Funny enough, the first 1 component get the job done with about 98% variance. I would still pick the first 3 components as it levels off right after, which means that they are not as significant as the other components.

{{< pagebreak >}}

## Question 5

### Part A


```{r}
nyseData <- read.csv("NYSEData.csv")

# Construct the sample covariance matrix S and find the sample principal components
covMatrix <- cov(nyseData)
print(covMatrix)
pca <- princomp(covMatrix)
print(pca)
```



{{< pagebreak >}}

## Question 5

### Part B


```{r}
# Calculate the proportion of total sample variance explained by the first three principal components
propVarExplained <- cumsum(pca$sdev^2) / sum(pca$sdev^2)
print(propVarExplained[3])
```



{{< pagebreak >}}

## Question 5

### Part C

-   In Principal Component 1, JPMorgan and Citibank show positive loadings, implying a strong positive association, while RoyalDutchShell and ExxonMobil exhibit negative loadings, indicating an inverse relationship with financial stocks. WellsFargo has a smaller positive loading, suggesting a positive association with less influence compared to JPMorgan and Citibank. 
-   In Principal Component 2, Citibank and ExxonMobil have negative and positive loadings, respectively, suggesting an inverse relationship between these financial and energy stocks. 
-   In Principal Component 3, JPMorgan and Citibank display positive loadings, indicating a positive association, while WellsFargo has a negative loading, suggesting a contrasting movement compared to JPMorgan and Citibank. RoyalDutchShell and ExxonMobil have small loadings, indicating less influence on this component.


```{r}
# Extract loadings for the first three components
loadings <- pca$loadings[, 1:3]
print(loadings)
```


