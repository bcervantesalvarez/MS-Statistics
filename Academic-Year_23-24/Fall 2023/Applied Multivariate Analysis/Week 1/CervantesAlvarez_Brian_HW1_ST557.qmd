---
title: "ST557: Homework 1"
author: 
    - "Brian Cervantes Alvarez"
date: "10-06-2023"
format: PrettyPDF-pdf
execute: 
  warning: false
  message: false
editor: visual
---

# Question 1

```{r}
# Set random seed for reproducibility
set.seed(503)

# Read HW1Q1 dataset in using base R
ds <- read.csv("HW1Q1.csv")
# Show first 5 rows
print(head(ds,50))

```

{{< pagebreak >}}

## Part A

Read this data into R using read.csv(). Create a 2-dimensional scatter plot of the 20 observations (use plot() function in R).

```{r}
# Get X and Y Components into vectors
x = ds$X
y = ds$Y

# Create scatter plot of Y ~ X
plot(x,y)
```

{{< pagebreak >}}

## Part B

Find the sample mean vector, and add this point to the plot using points(). You can make this point a different color using the col= argument, or you can make it a different plotting character using the pch= argument. For example: \> points(sampMean\[1\], sampMean\[2\], pch=16, col=2

```{r}
plot(x,y)

sampMeanX = mean(x)
sampMeanY = mean(y)

points(sampMeanX, sampMeanY, pch=16, col=2)

```

{{< pagebreak >}}

## Part C

Find the sample covariance matrix.

```{r}
# Sample covariance matrix
sampCov <- cov(ds)

sampCov
```

{{< pagebreak >}}

## Part D

Find the eigendecomposition (spectral decomposition) of the sample covariance matrix using eigen().

```{r}
sampEigenDecomp <- eigen(sampCov)

sampEigenDecomp
```

{{< pagebreak >}}

## Part E

Add the eigenvector corresponding to the largest eigenvalue to the plot as a vector from the sample mean using `lines()`. Be careful here: if the first eigenvector is $(v_1, v_2)$ and the sample mean vector is $(\bar{x}_1, \bar{x}_2)$, you want a line from $(\bar{x}_1, \bar{x}_2)$ to $(\bar{x}_1 + v_1, \bar{x}_2 + v_2)$."

```{r}
# Find the index of the largest eigenvalue
largestEigenvalue <- which.max(sampEigenDecomp$values)
# Extract the eigenvector corresponding to the largest eigenvalue
largestEigenvector <- sampEigenDecomp$vectors[, largestEigenvalue]

# Calculate the endpoints of the line segment
sampMeanX <- mean(x)
sampMeanY <- mean(y)
xend <- sampMeanX + largestEigenvector[1]
yend <- sampMeanY + largestEigenvector[2]

plot(x,y)
# Add the line segment to the plot
lines(c(sampMeanX, xend), c(sampMeanY, yend), col = "seagreen4", lwd = 3)
points(sampMeanX, sampMeanY, pch=16, col=2)

```

### Discussion for Part E

**Question: Describe how the direction of this eigenvector relates to the cloud of data points**

The eigenvector follows the trend of the data points, which are showing a positive increasing trend, and it culminates precisely at the location of the sample means for X and Y.

{{< pagebreak >}}

# Question 2

## Part A

```{r}
ds <- read.csv("CubitData.csv")
head(ds,5)

# Calculate the sample mean vector
sampleMeanVec <- colMeans(ds)
sampleMeanVec
```

{{< pagebreak >}}

## Part B

```{r}
# Calculate the sample covariance matrix!
sampleCovMatrix <- cov(ds)
sampleCovMatrix
```

{{< pagebreak >}}

## Part C

```{r}
# Calculate the eigendecomposition using the covariance matrix, sampleCovMatrix
sampleEigenDecomp <- eigen(sampleCovMatrix)
sampleEigenDecomp
```

{{< pagebreak >}}

## Part D

```{r}
# Find the index of the largest eigenvalue
largestEigenvalue <- which.max(sampleEigenDecomp$values)
# Extract the eigenvector corresponding to the largest eigenvalue
largestEigenvector <- sampleEigenDecomp$vectors[, largestEigenvalue]
largestEigenvector
```

{{< pagebreak >}}

```{r}
# Create the scatter plot
plot(ds$height, ds$cubit)

# Calculate each sample mean
meanCubit <- mean(ds$cubit)
meanHeight <- mean(ds$height)

# Use Part D!

xend <- meanHeight + largestEigenvector[1]
yend <- meanCubit + largestEigenvector[2]


# Plot the eigenvector as a line from the mean point
lines(c(meanHeight, xend), c(meanCubit, yend), col = "seagreen4", lwd = 3)

```

### Discussion for Part E

**Question: Describe how the direction of this eigenvector relates to the cloud of data points**

The eigenvector for the height and cubit follows the trend of the data points, which are showing a positive increasing trend. While not exactly the same, it's similar to problem 1.

{{< pagebreak >}}

# Question 3

## Part A

```{r}

A <- matrix(c(5.125, 3.875, 2.125, -1.125, 0,
              3.875, 5.125, -1.125, 2.125, 0,
              2.125, -1.125, 5.125, 3.875, 0,
              -1.125, 2.125, 3.875, 5.125, 0,
              0, 0, 0, 0, -3), 
            nrow = 5, 
            byrow = TRUE)

# Calculate the eigendecomposition
eigenDecomposition <- eigen(A)
eigenDecomposition

```

{{< pagebreak >}}

## Part B

```{r}
# Grab the eigenvalues from the eigen decomposition
eigenvalues <- eigenDecomposition$values
# Use the function "all" to check all eigenvalues to see if they're positive definite
isPositiveDefinite <- all(eigenvalues > 0)
isPositiveDefinite

# False! Now we need to find a vector x...

# Get all eigenvector(s) that are negative
negVec <- eigenDecomposition$vectors[, which(eigenvalues < 0)]

# We need to normalize the eigenvector(s) using the equation 
vecNorm <- negVec / sqrt(sum(negVec^2))
vecNorm

result <- t(vecNorm) %*% A %*% vecNorm
result
```

We end of getting a vector $x = \begin{bmatrix} 0.3535534 \\ -0.3535534 \\ -0.3535534 \\ 0.3535534 \\ 0 \end{bmatrix}$ for which $x^T A x < 0$, confirming that $A$ is not positive definite.

{{< pagebreak >}}

## Part C

The matrix-vector multiplication $Ax$, where $x = 4v_1 + 2v_5$, can be expressed symbolically as:

$$Ax = 4\lambda_1 v_1 + 2\lambda_5 v_5$$

In this expression, $\lambda_1$ and $\lambda_5$ are the eigenvalues corresponding to $v_1$ and $v_5$, respectively. If we want to calculate it, we could plug in the eigenvalues and solve.

{{< pagebreak >}}

# Question 4

## Part A

```{r}
# Read the data from the CSV file
ds <- read.csv("IrisData.csv")
names(ds)

# Calculate the sample mean vector for all variables
meanVec <- colMeans(ds[,])
meanVec

print(ds$Type)
```

{{< pagebreak >}}

## Part B

```{r}
# Calculate the sample mean vector for each species
species1MeanVec <- colMeans(ds[ds$type == 1, 1:4])
species2MeanVec <- colMeans(ds[ds$type == 2, 1:4])
species3MeanVec <- colMeans(ds[ds$type == 3, 1:4])

```

{{< pagebreak >}}

## Part C

```{r}
# Calculate the sample correlation matrix for all variables
corMatrix <- cor(ds[,])
corMatrix

```

{{< pagebreak >}}

## Part D

```{r}
# Calculate individual correlation matrices for each species
species1CorMatrix <- cor(ds[ds$type == 1, 1:4])
species2CorMatrix <- cor(ds[ds$type == 2, 1:4])
species3CorMatrix <- cor(ds[ds$type == 3, 1:4])
```

{{< pagebreak >}}

## Part E

```{r}
library(ggplot2)

pairs(ds[, 1:4], 
      main = "Pairs Plot for Iris Dataset",
      pch = 21, col = as.numeric(ds$Type), 
      labels = colnames(ds)[1:4],
      lower.panel = panel.smooth,
)
```

You can totally spot some noticeable differences among these flowers. When it comes to telling them apart, Petal Width seems to be the way to go, thanks to their clear clustering that lets you easily identify the species. And if you look at Petal Length and Petal Width, it's pretty apparent that they both follow a nice linear pattern in their clustering, making it even easier to tell them apart based on these features.

{{< pagebreak >}}

# Question 5

# Part A

To show that B is a symmetric matrix, we need to demonstrate that B equals its transpose, i.e., B = B\^T. We have:

$$
B = A^T A
$$

Taking the transpose of B:

$$
B^T = (A^T A)^T = A^T (A^T)^T = A^T A = B
$$

Thus, B is a symmetric matrix.

{{< pagebreak >}}

# Part B

To show that B is a positive semi-definite matrix, we need to prove that for any vector $x$ in $\mathbb{R}^p$, $x^T B x \geq 0$. Let's calculate this expression:

$$
x^T B x = x^T (A^T A) x = (x^T A^T) (A x) = (Ax)^T (Ax) = \lVert Ax \rVert^2
$$

Since the square of the Euclidean norm (length) of any vector is non-negative ($\lVert v \rVert^2 \geq 0$ for any vector $v$), we have $x^T B x \geq 0$. Therefore, B is a positive semi-definite matrix.

{{< pagebreak >}}

# Part C

The sample covariance matrix S can be expressed as:

$$
S = \frac{1}{n-1} (X - \bar{X})^T (X - \bar{X})
$$

Where $\bar{X}$ is an $(n \times p)$ matrix with n identical rows equal to the sample mean vector $(\bar{X}_1, \bar{X}_2, \ldots, \bar{X}_p)$. To show that S is positive semi-definite, we need to prove that for any vector $x$ in $\mathbb{R}^p$, $x^T S x \geq 0$.

Let $y = (X - \bar{X})x$, which is a linear combination of the columns of $X - \bar{X}$. Now, the expression $x^T S x$ can be written as:

$$
x^T S x = \frac{1}{n-1} y^T y
$$

Since $y$ is a linear combination of the columns of $X - \bar{X}$, $y$ is a vector in $\mathbb{R}^n$. The squared norm of any vector in $\mathbb{R}^n$ is non-negative. Therefore, $\frac{1}{n-1} y^T y \geq 0$, and as a result, \$x\^T S x \geq 0.

This shows that the sample covariance matrix S is positive semi-definite.
