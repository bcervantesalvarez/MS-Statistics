---
title: "Probability Theory Homework Solutions"
subtitle: "MTH 664"
author: "Brian Cervantes Alvarez"
date: today
format: pdf
toc: true
editor:
  render-on-save: true
---

{{< pagebreak >}}

# Chapter 1.1 Problems

## **Hints for Exercises**

### **Exercise 1.1.1**

- This exercise involves verifying that $(\mathbb{R}, \mathcal{F}, P)$ forms a valid probability space.
- Review **Section 1.1** on **Probability Spaces**, particularly the conditions for a probability measure.
- Refer to the definition of a **$\sigma$-algebra** and a **probability measure** in the beginning of **Section 1.1**.

### **Exercise 1.1.2**

- This question asks you to show that the $\sigma$-algebra generated by the semialgebra $\mathcal{S}_d$ is the Borel subsets of $\mathbb{R}^d$.
- Refer to **Example 1.1.5** for the definition of the semialgebra $\mathcal{S}_d$ and **Theorem 1.1.9** for extending measures from a semialgebra to a $\sigma$-algebra.
- Review the explanation of the **Borel sets** in **Section 1.1**.

### **Exercise 1.1.3**

- This exercise asks you to prove that the $\sigma$-field $\mathbb{R}^d$ is countably generated.
- Refer to the definition of **countably generated $\sigma$-fields** in **Section 1.1**.
- You can use the idea from **Example 1.1.3** (measures on the real line) and **Example 1.1.12** (product measures) to understand how countable collections can generate the Borel $\sigma$-field.

### **Exercise 1.1.4**

- This problem asks you to show that the union of an increasing sequence of $\sigma$-algebras forms an algebra but not necessarily a $\sigma$-algebra.
- Review the properties of **$\sigma$-algebras** and **algebras** in **Example 1.1.6** and **Lemma 1.1.7**.
- To give a counterexample in part (ii), look at how $\sigma$-algebras are closed under countable unions and intersections, which might not hold in the case of algebras.

### **Exercise 1.1.5**

- This exercise focuses on the concept of **asymptotic density** and whether the collection of sets with this property forms a $\sigma$-algebra or an algebra.
- For insights into this, refer to the definition of **algebras** and **$\sigma$-algebras** in **Section 1.1**, especially **Example 1.1.6** for a concrete example of an algebra.
- Pay attention to whether the properties of asymptotic density hold under complement and union operations.

{{< pagebreak >}}

## 1.1.1

**1.1.1.** Let $\Omega = \mathbb{R}$, $\mathcal{F}$ be all subsets so that $A$ or $A^c$ is countable, $P(A) = 0$ in the first case and $= 1$ in the second. Show that $(\Omega, \mathcal{F}, P)$ is a probability space.

**Proof**

Let $\Omega = \mathbb{R}$, $\mathcal{F} = \{ A \subseteq \Omega : A \text{ or } A^c \text{ is countable} \}$, and define $P: \mathcal{F} \to [0,1]$ by
$$
P(A) =
\begin{cases}
0, & \text{if } A \text{ is countable}, \\
1, & \text{if } A^c \text{ is countable}.
\end{cases}
$$

**1. $\mathcal{F}$ is a $\sigma$-algebra:**

(i) $\Omega \in \mathcal{F}$: Since $\Omega^c = \emptyset$ is countable, $\Omega \in \mathcal{F}$.

(ii) Closed under complement: For $A \in \mathcal{F}$, if $A$ is countable, then $A^c$ is uncountable, so $A^c \in \mathcal{F}$. If $A^c$ is countable, then $A$ is uncountable, and $A^c \in \mathcal{F}$.

(iii) Closed under countable unions: Let $\{A_n\}_{n=1}^\infty \subseteq \mathcal{F}$.

  - If all $A_n$ are countable, then $\bigcup_{n=1}^\infty A_n$ is countable, so $\bigcup_{n=1}^\infty A_n \in \mathcal{F}$.

  - If at least one $A_n^c$ is countable, say $A_{n_0}^c$, then $A_{n_0}$ is co-countable. Since $A_{n_0} \subseteq \bigcup_{n=1}^\infty A_n$, $(\bigcup_{n=1}^\infty A_n)^c \subseteq A_{n_0}^c$ is countable. Thus, $\bigcup_{n=1}^\infty A_n$ is co-countable, so $\bigcup_{n=1}^\infty A_n \in \mathcal{F}$.

**2. $P$ is a probability measure:**

(i) Non-negativity: $P(A) \in \{0,1\}$.

(ii) Normalization: $P(\Omega) = 1$ since $\Omega^c$ is countable.

(iii) Countable additivity: For disjoint $\{A_n\}_{n=1}^\infty \subseteq \mathcal{F}$:

  - If $\bigcup_{n=1}^\infty A_n$ is countable, then $P\left(\bigcup_{n=1}^\infty A_n\right) = 0$, and $P(A_n) = 0$ for all $n$, so
    $$
    P\left( \bigcup_{n=1}^\infty A_n \right) = \sum_{n=1}^\infty P(A_n) = 0.
    $$

  - If $(\bigcup_{n=1}^\infty A_n)^c$ is countable, then $P\left(\bigcup_{n=1}^\infty A_n\right) = 1$. Since the $A_n$ are disjoint and their union is co-countable, at least one $A_n$ is co-countable ($P(A_n) = 1$), and the rest are countable ($P(A_n) = 0$). Thus,
    $$
    P\left( \bigcup_{n=1}^\infty A_n \right) = \sum_{n=1}^\infty P(A_n) = 1.
    $$

Therefore, $(\Omega, \mathcal{F}, P)$ is a probability space.

{{< pagebreak >}}

## 1.1.2

**1.1.2.** Recall the definition of $S_d$ from Example 1.1.5. Show that $\sigma(S_d) = \mathcal{R}^d$, the Borel subsets of $\mathbb{R}^d$.

**Proof**

Let $S_d = \{ \{ x \in \mathbb{R}^d : x_i \leq a \} : a \in \mathbb{R},\ i = 1,\dots,d \}$.

**1. $S_d \subseteq \mathcal{R}^d$:** Each set in $S_d$ is a closed half-space, hence closed, so $S_d \subseteq \mathcal{R}^d$.

**2. $\sigma(S_d) \subseteq \mathcal{R}^d$:** Since $\mathcal{R}^d$ is a $\sigma$-algebra containing $S_d$, it contains $\sigma(S_d)$.

**3. $\mathcal{R}^d \subseteq \sigma(S_d)$:**

- Any closed rectangle $R = \prod_{i=1}^d [a_i, b_i]$ can be expressed as an intersection of sets from $S_d$ and their complements.

- Closed rectangles generate the Borel $\sigma$-algebra $\mathcal{R}^d$.

Therefore, $\sigma(S_d) = \mathcal{R}^d$.

{{< pagebreak >}}

## 1.1.3

**1.1.3.** A $\sigma$-field $\mathcal{F}$ is said to be *countably generated* if there is a countable collection $C \subset \mathcal{F}$ so that $\sigma(C) = \mathcal{F}$. Show that $\mathbb{R}^d$ is countably generated.

**Proof**

Let $\mathcal{C} = \{ B(x, r) : x \in \mathbb{Q}^d,\ r \in \mathbb{Q}^+ \}$, where $B(x, r) = \{ y \in \mathbb{R}^d : \| y - x \| < r \}$.

**1. $\mathcal{C}$ is countable:** $\mathbb{Q}^d$ and $\mathbb{Q}^+$ are countable; their product is countable.

**2. $\mathcal{C}$ is a basis:** Open sets in $\mathbb{R}^d$ can be written as unions of balls in $\mathcal{C}$.

**3. $\mathcal{R}^d = \sigma(\mathcal{C})$:** Since $\mathcal{R}^d$ is generated by open sets, and open sets are unions of elements of $\mathcal{C}$, $\mathcal{R}^d$ is countably generated.

{{< pagebreak >}}

## 1.1.4

**1.1.4.** (i) Show that if $\mathcal{F}_1 \subset \mathcal{F}_2 \subset \dots$ are $\sigma$-algebras, then $\cup_i \mathcal{F}_i$ is an algebra. (ii) Give an example to show that $\cup_i \mathcal{F}_i$ need not be a $\sigma$-algebra.

**Proof**

Let $\{\mathcal{F}_n\}_{n=1}^\infty$ be increasing $\sigma$-algebras, i.e., $\mathcal{F}_n \subseteq \mathcal{F}_{n+1}$.

Define $\mathcal{A} = \bigcup_{n=1}^\infty \mathcal{F}_n$.

Closure under complement:** For $A \in \mathcal{A}$, $A \in \mathcal{F}_k$ for some $k$. Since $\mathcal{F}_k$ is a $\sigma$-algebra, $A^c \in \mathcal{F}_k \subseteq \mathcal{A}$.

Closure under finite unions:** For $A, B \in \mathcal{A}$, $A \in \mathcal{F}_k$, $B \in \mathcal{F}_m$. Let $n = \max\{k, m\}$. Then $A, B \in \mathcal{F}_n$, so $A \cup B \in \mathcal{F}_n \subseteq \mathcal{A}$.

Therefore, $\mathcal{A}$ is an algebra.

**Example**

Let $\Omega = [0,1]$, and $\mathcal{F}_n = \sigma(\{ [0, 1 - \tfrac{1}{n}] \})$.

- Each $\mathcal{F}_n$ is a $\sigma$-algebra on $[0,1 - \tfrac{1}{n}]$.

- The union $\mathcal{A} = \bigcup_{n=1}^\infty \mathcal{F}_n$ does not contain $[0,1)$, since $[0,1) \notin \mathcal{F}_n$ for any $n$.

- $\mathcal{A}$ is not a $\sigma$-algebra because it is not closed under countable unions, e.g., $\bigcup_{n=1}^\infty [0, 1 - \tfrac{1}{n}] = [0,1)$ is not in $\mathcal{A}$.

{{< pagebreak >}}

## 1.1.5

**1.1.5.** A set $A \subset \{1, 2, \dots\}$ is said to have *asymptotic density* $\theta$ if
$$
\lim_{n \to \infty} \frac{|A \cap \{1, 2, \dots, n\}|}{n} = \theta.
$$
Let $\mathcal{A}$ be the collection of sets for which the asymptotic density exists. Is $\mathcal{A}$ a $\sigma$-algebra? An algebra?

**Proof**

Let $\mathcal{A} = \{ A \subseteq \mathbb{N} : d(A) \text{ exists} \}$, where
$$
d(A) = \lim_{n \to \infty} \frac{|A \cap \{1,2,\dots,n\}|}{n}.
$$

**1. $\mathcal{A}$ is an algebra:**

Closure under complement: If $d(A) = \theta$, then $d(A^c) = 1 - \theta$, so $A^c \in \mathcal{A}$.

Closure under finite unions: For $A, B \in \mathcal{A}$, $d(A \cup B) = d(A) + d(B) - d(A \cap B)$ exists, so $A \cup B \in \mathcal{A}$.

**2. $\mathcal{A}$ is not a $\sigma$-algebra:**

- Consider $A_n = \{ n \}$, each with $d(A_n) = 0$.

- The union $\bigcup_{n=1}^\infty A_n = \mathbb{N}$, so $d\left( \bigcup_{n=1}^\infty A_n \right) = 1$.

- Since countable unions may not preserve the existence of $d(A)$, $\mathcal{A}$ is not a $\sigma$-algebra.


{{< pagebreak >}}

# Chapter 1.2 Problems

## **Hints for Exercises**

### **Exercise 1.2.1**

- This exercise asks you to show that $Z(\omega)$, defined as a piecewise function based on the values of $X$ and $Y$, is a random variable.
- Refer to the definition of a **random variable** and $\mathcal{F}$-measurability in **Section 1.2**, specifically the part where $X$ is said to be a random variable if $X^{-1}(B) \in \mathcal{F}$ for Borel sets $B$.
- Also, the **indicator function** (discussed in **Figure 1.3**) could help here since $Z$ depends on conditions related to events in $\mathcal{F}$.

### **Exercise 1.2.2**

- To solve this, use **Theorem 1.2.6** from the section on the **standard normal distribution** to find bounds on $P(\chi \geq 4)$.
- Review the **integral bounds** given in Theorem 1.2.6 and apply them with the appropriate limits to compute these probabilities.
- For the **standard normal distribution** $f(x)$, review **Example 1.2.5**.

### **Exercise 1.2.3**

- This exercise asks you to prove that a distribution function can only have countably many discontinuities.
- Refer to the properties of the **distribution function** $F(x)$ outlined in **Theorem 1.2.1**, particularly the facts that $F$ is **nondecreasing** and **right continuous**.
- Use the fact that **nondecreasing functions** can only have jump discontinuities, and any function with more than countably many jumps would not be right continuous.

### **Exercise 1.2.4**

- This exercise explores the connection between a distribution function and the **uniform distribution** on $(0, 1)$.
- Review the concept of **distribution functions** (refer to **Theorem 1.2.1**), especially the result about inducing a probability measure on $R$.
- Recall the connection between **random variables** and **uniform distributions**, as seen in **Example 1.2.3**. Consider the inverse of the distribution function and use the properties of **$F(X)$ as a uniform random variable**.

### **Exercise 1.2.5**

- To solve this, you need to transform a random variable using a strictly increasing and differentiable function $g$.
- Use the **change of variables formula** for random variables (refer to **Example 1.2.5** where transformations are introduced) to express the density of $g(X)$ in terms of the original density $f$ of $X$.
- For an increasing function $g$, the key formula here is:
$$
f_{g(X)}(y) = \frac{f_X(g^{-1}(y))}{g'(g^{-1}(y))}.
$$

This can be derived from the section discussing **inverse distributions** (refer to **Figure 1.4**).

### **Exercise 1.2.6**

- This question asks you to compute the density of $Y = \exp(X)$ when $X$ has a normal distribution.
- Use the result from **Exercise 1.2.5** and apply it to the exponential transformation $g(X) = \exp(X)$.
- Remember, when $g(X) = e^X$, then $g^{-1}(y) = \log(y)$, and the derivative $g'(X) = e^X$. This will lead you to the density of the **lognormal distribution**.

### **Exercise 1.2.7**

- Part (i) asks for the distribution of $X^2$ if $X$ has a given density function $f$. You will need to apply the **transformation technique** (similar to Exercise 1.2.5) to compute the distribution of $X^2$.
- Start by computing the cumulative distribution function (CDF) $F(x)$ of $X^2$, and then differentiate to get the **density function**.
- In part (ii), you are asked to find the density of the **chi-square distribution** when $X$ is a standard normal random variable. Use the fact that the **chi-square distribution** with one degree of freedom is the distribution of the square of a standard normal variable.



{{< pagebreak >}}

## 1.2.1

**1.2.1.** Suppose $X$ and $Y$ are random variables, $A \in \mathcal{F}$. Define $Z(\omega) = X(\omega)$ if $\omega \in A$, $Z(\omega) = Y(\omega)$ if $\omega \in A^c$. Show that $Z$ is a random variable.

**Proof**

For any Borel set $B \subseteq \mathbb{R}$, the preimage is
$$
Z^{-1}(B) = [A \cap X^{-1}(B)] \cup [A^c \cap Y^{-1}(B)].
$$
Since $X$ and $Y$ are measurable, $X^{-1}(B), Y^{-1}(B) \in \mathcal{F}$. Thus, $Z^{-1}(B) \in \mathcal{F}$, so $Z$ is measurable.


{{< pagebreak >}}

## 1.2.2

**1.2.2.** Let $\chi$ have a standard normal distribution. Use Theorem 1.2.6 to get upper and lower bounds on $P(\chi \geq 4)$.

**Proof**

From Theorem 1.2.6:
$$
\frac{1}{\sqrt{2\pi}} \frac{x}{x^2 + 1} e^{-x^2/2} \leq P(\chi \geq x) \leq \frac{1}{\sqrt{2\pi} x} e^{-x^2/2}.
$$
For $x = 4$:

- Upper bound:
$$
P(\chi \geq 4) \leq \frac{1}{4 \sqrt{2\pi}} e^{-8}.
$$

- Lower bound:
$$
P(\chi \geq 4) \geq \frac{4}{17 \sqrt{2\pi}} e^{-8}.
$$


{{< pagebreak >}}

## 1.2.3

**1.2.3.** Show that a distribution function has at most countably many discontinuities.

**Proof**

Let $F$ be a distribution function, and let
$$
D = \{ x \in \mathbb{R} : F \text{ is discontinuous at } x \}.
$$
For each $x \in D$, define the jump $\delta_x = F(x) - F(x^-)$, where $F(x^-)$ is the left-hand limit of $F$ at $x$. Since $F$ is non-decreasing and right-continuous, each $\delta_x > 0$.

The total variation of $F$ over $\mathbb{R}$ satisfies
$$
\sum_{x \in D} \delta_x \leq F(\infty) - F(-\infty) = 1 - 0 = 1.
$$
Because the sum of the positive numbers $\delta_x$ over $D$ is finite (at most 1), the set $D$ must be at most countable.


{{< pagebreak >}}

## 1.2.4

**1.2.4.** If $F(x) = P(X \leq x)$ is continuous, then $Y = F(X)$ is uniformly distributed on $(0,1)$.

**Proof**

For $y \in (0,1)$:
$$
P(Y \leq y) = P(F(X) \leq y) = P(X \leq F^{-1}(y)) = F(F^{-1}(y)) = y.
$$
Thus, $Y$ has the uniform distribution on $(0,1)$.


{{< pagebreak >}}

## 1.2.5

**1.2.5.** Suppose $X$ has continuous density $f$ with support $[\alpha, \beta]$, and $g$ is strictly increasing and differentiable on $(\alpha, \beta)$. Show that $Y = g(X)$ has density $h(y) = \dfrac{f(g^{-1}(y))}{g'(g^{-1}(y))}$.

**Proof**

For $y \in (g(\alpha), g(\beta))$, the density of $Y$ is:
$$
h(y) = f(g^{-1}(y)) \left| \dfrac{d}{dy} g^{-1}(y) \right| = \dfrac{f(g^{-1}(y))}{g'(g^{-1}(y))}.
$$
For $y \notin (g(\alpha), g(\beta))$, $h(y) = 0$.


{{< pagebreak >}}

## 1.2.6

**1.2.6.** Suppose $X$ has a normal distribution. Compute the density of $Y = \exp(X)$.

**Proof**

Let $X \sim N(\mu, \sigma^2)$, $g(x) = \exp(x)$. Then $g^{-1}(y) = \ln(y)$, $g'(x) = \exp(x) = y$.

The density of $Y$ is:
$$
h(y) = \dfrac{f_X(\ln(y))}{y} = \dfrac{1}{y \sqrt{2\pi \sigma^2}} \exp\left( -\dfrac{(\ln y - \mu)^2}{2\sigma^2} \right), \quad y > 0.
$$
For $y \leq 0$, $h(y) = 0$.


{{< pagebreak >}}

## 1.2.7

**1.2.7.** (i) Suppose $X$ has density $f$. Compute the distribution function and density of $Y = X^2$. (ii) When $X$ is standard normal, find the density of $Y$.

**Proof**

**(i)**

For $y \geq 0$:
$$
F_Y(y) = P(X^2 \leq y) = P(-\sqrt{y} \leq X \leq \sqrt{y}) = F_X(\sqrt{y}) - F_X(-\sqrt{y}).
$$
The density is:
$$
f_Y(y) = \dfrac{d}{dy} F_Y(y) = \dfrac{f_X(\sqrt{y}) + f_X(-\sqrt{y})}{2\sqrt{y}}.
$$

**(ii)**

Let $X \sim N(0,1)$, so $f_X(x) = \dfrac{1}{\sqrt{2\pi}} e^{-x^2/2}$.

Then for $y > 0$:
$$
f_Y(y) = \dfrac{f_X(\sqrt{y}) + f_X(-\sqrt{y})}{2\sqrt{y}} = \dfrac{1}{\sqrt{2\pi y}} e^{-y/2}.
$$
Thus, $Y$ has the chi-square distribution with 1 degree of freedom.


{{< pagebreak >}}

# Chapter 1.3 Problems

## **Hints for Exercises**

### **Exercise 1.3.1**
- **Hint**: This exercise asks you to show that if $A$ generates $S$, then $X^{-1}(A) \in \mathcal{F}$ for all $A \in \mathcal{A}$. 
- **Refer to**: Theorem 1.3.1 (Page 15), which provides the condition for $X$ being measurable. Also, consider how the $\sigma$-field generated by a set is defined.

### **Exercise 1.3.2**
- **Hint**: You're asked to prove Theorem 1.3.6 when $n = 2$. 
- **Refer to**: Theorem 1.3.6 (Page 16), which deals with the sum of random variables. Consider using Example 1.3.2 for help with open sets, and note the properties of measurable maps.

### **Exercise 1.3.3**
- **Hint**: You need to show that if $f$ is continuous and $X_n \to X$ almost surely, then $f(X_n) \to f(X)$ almost surely.
- **Refer to**: Theorem 1.3.4 (Page 15), which discusses how a function of random variables is measurable. Use the continuity of $f$ and the almost-sure convergence of $X_n \to X$.

### **Exercise 1.3.4**
- **Part (i)**: You are asked to show that a continuous function from $\mathbb{R}^d$ to $\mathbb{R}$ is measurable.
    - **Refer to**: Theorem 1.3.4 (Page 15), which provides a direct result on measurable functions when $f$ is a continuous function.
  
- **Part (ii)**: Show that $\mathbb{R}^d$ is the smallest $\sigma$-field that makes all continuous functions measurable.
    - **Hint**: Use the definition of the $\sigma$-field generated by a collection of sets and how measurable functions behave over that field.

### **Exercise 1.3.5**
- **Hint**: You are asked to show that a function $f$ is lower semicontinuous if $\liminf_{y \to x} f(y) \geq f(x)$.
- **Refer to**: Definitions on Page 17 of lower and upper semicontinuous functions. Carefully apply the conditions given in the exercise to prove both statements.

### **Exercise 1.3.6**
- **Hint**: This involves showing that a function is measurable using properties of sup and inf. Specifically, how they relate to the distance function in $\mathbb{R}^d$.
- **Refer to**: Definition of lower and upper semicontinuous functions (Page 17). Use the properties of limits and apply them to the sup and inf operations.

### **Exercise 1.3.7**
- **Hint**: This exercise is about simple functions and their representation as sums of indicator functions.
- **Refer to**: Page 17, where simple functions are defined and shown as sums of indicator functions. The definition will help you construct the required proof.

### **Exercise 1.3.8**
- **Hint**: Here, you're concluding that $Y$ is measurable with respect to $\sigma(X)$ if and only if $Y = f(X)$ where $f$ is a measurable function.
- **Refer to**: Use Exercise 1.3.7 and the definition of $\sigma(X)$ on Page 15 to guide your solution.

### **Exercise 1.3.9**
- **Hint**: This is a constructive proof showing the relationship between measurable sets and limits.
- **Refer to**: Definition of $\sigma(X)$ (Page 17) and measurable functions. Also, apply concepts from limits and how they interact with the measurable space.

{{< pagebreak >}}

## 1.3.1

**1.3.1.** Show that if $\mathcal{A}$ generates $\mathcal{S}$, then $X^{-1}(\mathcal{A}) \equiv \{\{X \in A\} : A \in \mathcal{A}\}$ generates $\sigma(X) = \{\{X \in B\} : B \in \mathcal{S}\}$.

**Proof**

Since $\mathcal{A}$ generates $\mathcal{S}$, we have $\sigma(\mathcal{A}) = \mathcal{S}$. Consider the preimage under $X$:

$$
\sigma\big(X^{-1}(\mathcal{A})\big) = \{X^{-1}(B) : B \in \sigma(\mathcal{A})\} = \{X^{-1}(B) : B \in \mathcal{S}\} = \sigma(X).
$$

Thus, $X^{-1}(\mathcal{A})$ generates $\sigma(X)$.

{{< pagebreak >}}

## 1.3.2

**1.3.2.** Prove Theorem 1.3.6 when $n = 2$ by checking $\{X_1 + X_2 \leq x\} \in \mathcal{F}$.

**Proof**

Since $X_1$ and $X_2$ are random variables, they are measurable functions from $(\Omega, \mathcal{F})$ to $(\mathbb{R}, \mathcal{R})$. The function $f: \mathbb{R}^2 \to \mathbb{R}$ defined by $f(x_1, x_2) = x_1 + x_2$ is continuous and therefore measurable.

The composition of measurable functions is measurable, so the function $Z = f(X_1, X_2) = X_1 + X_2$ is measurable. This means that for any Borel set $B \subseteq \mathbb{R}$,
$$
Z^{-1}(B) = \{ \omega \in \Omega : X_1(\omega) + X_2(\omega) \in B \} \in \mathcal{F}.
$$
Thus, $X_1 + X_2$ is a random variable.


{{< pagebreak >}}

## 1.3.3

**1.3.3** Show that if $f$ is continuous and $X_n \to X$ almost surely, then $f(X_n) \to f(X)$ almost surely.

**Proof**

For almost every $\omega$, $X_n(\omega) \to X(\omega)$. Since $f$ is continuous, it follows that $f(X_n(\omega)) \to f(X(\omega))$. Hence, $f(X_n) \to f(X)$ almost surely.

{{< pagebreak >}}

## 1.3.4

**1.3.4.** (i) Show that a continuous function from $\mathbb{R}^d \to \mathbb{R}$ is a measurable map from $(\mathbb{R}^d, \mathcal{R}^d)$ to $(\mathbb{R}, \mathcal{R})$. (ii) Show that $\mathcal{R}^d$ is the smallest $\sigma$-field that makes all the continuous functions measurable.

**Proof**

(i) For any open set $O \subseteq \mathbb{R}$, the preimage $f^{-1}(O)$ is open in $\mathbb{R}^d$ since $f$ is continuous. Therefore, $f$ is measurable.

(ii) The Borel $\sigma$-field $\mathcal{R}^d$ is generated by open sets. Since continuous functions pull back open sets to open sets, $\mathcal{R}^d$ is the smallest $\sigma$-field making all continuous functions measurable.

{{< pagebreak >}}

## 1.3.5

**1.3.5.** A function $f$ is said to be *lower semicontinuous* (l.s.c.) if $\liminf_{y \to x} f(y) \geq f(x),$ and *upper semicontinuous* (u.s.c.) if $-f$ is l.s.c. Show that $f$ is l.s.c. if and only if $\{x : f(x) \leq a\}$ is closed for each $a \in \mathbb{R}$, and conclude that semicontinuous functions are measurable.

**Proof**

$f$ is lower semicontinuous (l.s.c.) if and only if for all $a \in \mathbb{R}$, the set $\{ x : f(x) > a \}$ is open. Therefore, $\{ x : f(x) \leq a \}$ is closed, being the complement of an open set. Since closed sets are measurable, l.s.c. functions are measurable. Similarly, upper semicontinuous (u.s.c.) functions are measurable.

{{< pagebreak >}}

## 1.3.6

**1.3.6.** Let $f : \mathbb{R}^d \to \mathbb{R}$ be an arbitrary function and define

$$
f^\delta(x) = \sup\{f(y) : |y - x| < \delta\}, \quad f_\delta(x) = \inf\{f(y) : |y - x| < \delta\},
$$

where $|z| = \sqrt{z_1^2 + \dots + z_d^2}$. Show that $f^\delta$ is l.s.c. and $f_\delta$ is u.s.c. Let

$$
f^0 = \lim_{\delta \to 0} f^\delta, \quad f_0 = \lim_{\delta \to 0} f_\delta,
$$

and conclude that the set of points at which $f$ is discontinuous is $\{f^0 \neq f_0\}$, which is measurable.

**Proof**

Define $f^\delta(x) = \sup\{ f(y) : |y - x| < \delta \}$. To prove that $f^\delta$ is lower semicontinuous (l.s.c.), let $x \in \mathbb{R}^d$ and consider a sequence $\{x_n\}$ such that $x_n \to x$. By definition, $f^\delta(x_n)$ is the supremum over the ball $B_\delta(x_n)$, which approaches $B_\delta(x)$ as $n \to \infty$. Therefore, for any point $y$ satisfying $|y - x| < \delta$, eventually $y \in B_\delta(x_n)$ for large $n$. This guarantees that

$$
\liminf_{n \to \infty} f^\delta(x_n) \geq f^\delta(x).
$$

Thus, $f^\delta$ is l.s.c.

Next, define $f_\delta(x) = \inf\{ f(y) : |y - x| < \delta \}$. To show that $f_\delta$ is upper semicontinuous (u.s.c.), consider the same point $x \in \mathbb{R}^d$ with the sequence $\{x_n\} \to x$. Since $f_\delta(x_n)$ is the infimum over $B_\delta(x_n)$, the infimum cannot increase as $x_n \to x$. Therefore,

$$
\limsup_{n \to \infty} f_\delta(x_n) \leq f_\delta(x),
$$

proving that $f_\delta$ is u.s.c.

As $\delta \to 0$, the functions $f^\delta(x)$ decrease to $f^0(x) = \lim_{\delta \to 0} f^\delta(x)$, and the functions $f_\delta(x)$ increase to $f_0(x) = \lim_{\delta \to 0} f_\delta(x)$. The function $f$ is continuous at a point $x$ if and only if $f^0(x) = f_0(x) = f(x)$. Therefore, the set of discontinuity points of $f$ is precisely

$$
\{ x : f^0(x) \neq f_0(x) \}.
$$

Finally, since $f^\delta$ and $f_\delta$ are l.s.c. and u.s.c. functions, respectively, they are measurable. By the properties of pointwise limits, the functions $f^0$ and $f_0$ are also measurable. Thus, the set $\{ x : f^0(x) \neq f_0(x) \}$ is measurable, completing the proof.


{{< pagebreak >}}

## 1.3.7

**1.3.7.** A function $\varphi : \Omega \to \mathbb{R}$ is said to be simple if
$$
\varphi(\omega) = \sum_{m=1}^{n} c_m \mathbf{1}_{A_m}(\omega),
$$
where the $c_m$ are real numbers and $A_m \in \mathcal{F}$. Show that the class of $\mathcal{F}$-measurable functions is the smallest class containing the simple functions and closed under pointwise limits.

**Proof**

Let $\mathcal{S}$ be the set of simple functions, and let $\mathcal{M}$ be the set of $\mathcal{F}$-measurable functions.

1. **$\mathcal{S} \subseteq \mathcal{M}$:** Simple functions are measurable because each $\mathbf{1}_{A_m}$ is measurable (since $A_m \in \mathcal{F}$) and finite linear combinations of measurable functions are measurable.

2. **$\mathcal{M}$ is closed under pointwise limits:** If $\{ f_n \} \subseteq \mathcal{M}$ and $f_n \to f$ pointwise, then $f$ is measurable.

3. **Minimality:** Suppose $\mathcal{C}$ is a class containing $\mathcal{S}$ and closed under pointwise limits. Every measurable function $f$ can be approximated by simple functions $s_n \in \mathcal{S}$ such that $s_n \to f$ pointwise. Therefore, $f \in \mathcal{C}$, so $\mathcal{M} \subseteq \mathcal{C}$.

Thus, the class of $\mathcal{F}$-measurable functions is the smallest class containing the simple functions and closed under pointwise limits.


{{< pagebreak >}}

## 1.3.8

**1.3.8.** Use the previous exercise to conclude that $Y$ is measurable with respect to $\sigma(X)$ if and only if $Y = f(X)$ where $f : \mathbb{R} \to \mathbb{R}$ is measurable.

**Proof**

If $Y$ is measurable with respect to $\sigma(X)$, then for every Borel set $B \subseteq \mathbb{R}$, the preimage $Y^{-1}(B) \in \sigma(X)$. Since $\sigma(X)$ is generated by sets of the form $X^{-1}(A)$ for $A \in \mathcal{B}(\mathbb{R})$, there exists a measurable function $f : \mathbb{R} \to \mathbb{R}$ such that $Y = f(X)$.

Conversely, if $Y = f(X)$ with $f$ measurable, then for any Borel set $B \subseteq \mathbb{R}$,
$$
Y^{-1}(B) = \{ \omega \in \Omega : f(X(\omega)) \in B \} = X^{-1}(f^{-1}(B)) \in \sigma(X),
$$
since $f^{-1}(B)$ is a Borel set. Therefore, $Y$ is measurable with respect to $\sigma(X)$.

{{< pagebreak >}}

## 1.3.9

**1.3.9.** To get a constructive proof of the last result, note that $\{ \omega : m 2^{-n} \leq Y(\omega) < (m + 1) 2^{-n} \} = \{ \omega : X(\omega) \in B_{m,n} \}$ for some $B_{m,n} \in \mathcal{B}(\mathbb{R})$, and set $f_n(x) = m 2^{-n}$ for $x \in B_{m,n}$. Show that as $n \to \infty$, $f_n(x) \to f(x)$ and $Y = f(X)$.

**Proof**

For each $n \in \mathbb{N}$ and integers $m$, define intervals $I_{m,n} = [m 2^{-n}, (m + 1) 2^{-n})$. Define
$$
B_{m,n} = \{ x \in \mathbb{R} : P( X^{-1}(x) \cap Y^{-1}(I_{m,n}) ) > 0 \}.
$$
Since $Y$ is $\sigma(X)$-measurable, each set $Y^{-1}(I_{m,n}) \in \sigma(X)$, so there exist $B_{m,n} \in \mathcal{B}(\mathbb{R})$ such that
$$
Y^{-1}(I_{m,n}) = X^{-1}(B_{m,n}).
$$
Define the functions $f_n : \mathbb{R} \to \mathbb{R}$ by
$$
f_n(x) = m 2^{-n} \quad \text{if} \quad x \in B_{m,n}.
$$
Then,
$$
f_n(X(\omega)) = m 2^{-n} \quad \text{if} \quad X(\omega) \in B_{m,n},
$$
which implies
$$
m 2^{-n} \leq Y(\omega) < (m + 1) 2^{-n}.
$$
As $n \to \infty$, the difference $(m + 1) 2^{-n} - m 2^{-n} = 2^{-n}$ approaches zero, so
$$
\lim_{n \to \infty} f_n(X(\omega)) = Y(\omega).
$$
Define $f(x) = \lim_{n \to \infty} f_n(x)$. Since each $f_n$ is measurable and pointwise convergence preserves measurability, $f$ is measurable. Therefore, $Y = f(X)$ with $f : \mathbb{R} \to \mathbb{R}$ measurable.


{{< pagebreak >}}

# Chapter 1.4 Problems

## **Hints for Exercises**

### **Exercise 1.4.1**
- **Hint**: This exercise asks you to show that if $f \geq 0$ and $\int f \, d\mu = 0$, then $f = 0$ almost everywhere.
- **Refer to**: Lemma 1.4.3 (Page 20), specifically part (i), which states that if $f \geq 0$, then $\int f \, d\mu \geq 0$. Use the assumption that the integral is 0 and relate it to the definition of "almost everywhere."

### **Exercise 1.4.2**
- **Hint**: You need to prove the result involving the set $E_{n,m} = \{x : m / 2^n \leq f(x) < (m+1) / 2^n \}$ and show that the sum of measures of $E_{n,m}$ approximates the integral of $f$.
- **Refer to**: Step 2 of integration on Page 19. Pay attention to how integration is defined over bounded functions and how sup and inf approximations are used.

### **Exercise 1.4.3**
- **Hint**: This is a multi-part exercise about approximating an integrable function with a simple function $\varphi$, a step function $q$, and a continuous function $r$, all of which approximate $g$. 
  - **(i)**: Use the definition of simple functions to construct $\varphi$. 
  - **(ii)**: Use Exercise A.2.1 for finite unions of intervals.
  - **(iii)**: Round the corners of $q$ and replace it with a continuous function.
- **Refer to**: Step 1 and Step 3 (Pages 18-21), which discuss how integrable functions are approximated by simpler forms. Focus on how the supremum of simple functions is used in defining the integral.

### **Exercise 1.4.4**
- **Hint**: You are asked to prove the **Riemann-Lebesgue Lemma**, which states that if $g$ is integrable, then $\lim_{n \to \infty} \int g(x) \cos(nx) \, dx = 0$.
- **Refer to**: Lemma 1.4.4 (Page 22) and use the fact that if $g$ is a step function, the result is easy to prove. Then, follow the approximation process from earlier exercises to extend the result to more general integrable functions.


{{< pagebreak >}}

## 1.4.1

**1.4.1.** Show that if $f \geq 0$ and $\int f \, d\mu = 0$ then $f = 0$ a.e.

**Proof**

Let $f \geq 0$ and $\int f \, d\mu = 0$. Define the set $E = \{ x \in \Omega : f(x) > 0 \}$. Suppose, for contradiction, that $\mu(E) > 0$. Since $f(x) > 0$ on $E$, we have

$$
\int f \, d\mu = \int_{\Omega} f \, d\mu = \int_{E} f \, d\mu + \int_{\Omega \setminus E} f \, d\mu \geq \int_{E} f \, d\mu > 0,
$$

which contradicts $\int f \, d\mu = 0$. Therefore, $\mu(E) = 0$, and thus $f = 0$ almost everywhere.

{{< pagebreak >}}

## 1.4.2

**1.4.2.** Let $f \geq 0$ and $E_{n,m} = \{x : \frac{m}{2^n} \leq f(x) < \frac{m+1}{2^n}\}$. As $n \to \infty$,
$$
\sum_{m=1}^{\infty} \frac{m}{2^n} \mu(E_{n,m}) \uparrow \int f \, d\mu.
$$

**Proof**

Let $f \geq 0$ and define $E_{n,m} = \left\{ x : \dfrac{m}{2^n} \leq f(x) < \dfrac{m+1}{2^n} \right\}$ for integers $n \geq 1$ and $m \geq 0$. Consider the sum

$$
S_n = \sum_{m=0}^{\infty} \dfrac{m}{2^n} \mu(E_{n,m}).
$$

Since $f(x) \geq 0$, the sum starts from $m = 0$. For each $n$, the function

$$
s_n(x) = \sum_{m=0}^{\infty} \dfrac{m}{2^n} \mathbf{1}_{E_{n,m}}(x)
$$

is a simple function satisfying $0 \leq s_n(x) \leq f(x)$ for all $x$. Furthermore, as $n \to \infty$, $s_n(x) \uparrow f(x)$ pointwise. By the Monotone Convergence Theorem,

$$
\lim_{n \to \infty} S_n = \lim_{n \to \infty} \int s_n \, d\mu = \int \left( \lim_{n \to \infty} s_n(x) \right) d\mu = \int f \, d\mu.
$$

Therefore,

$$
\sum_{m=0}^{\infty} \dfrac{m}{2^n} \mu(E_{n,m}) \uparrow \int f \, d\mu \quad \text{as} \quad n \to \infty.
$$

{{< pagebreak >}}

## 1.4.3

**1.4.3.**  Let $g$ be an integrable function on $\mathbb{R}$ and $\epsilon > 0$. (i) Use the definition of the integral to conclude there is a simple function $\varphi = \sum_k b_k 1_{A_k}$ with $\int |g - \varphi| \, dx < \epsilon$. (ii) Use Exercise A.2.1 to approximate the $A_k$ by finite unions of intervals to get a step function
$$
q = \sum_{j=1}^{k} c_j 1_{(a_{j-1}, a_j)}.
$$
with $a_0 < a_1 < \dots < a_k$, so that $\int |\varphi - q| \, dx < \epsilon$. (iii) Round the corners of $q$ to get a continuous function $r$ so that $\int |q - r| \, dx < \epsilon$.

(iii) To make a continuous function, replace each $c_j 1_{(a_{j-1}, a_j)}$ by a function that is 0 on $(a_{j-1}, a_j)^c$, $c_j$ on $[a_{j-1} + \delta, a_j - \delta]$, and linear otherwise. If the $\delta_j$ are small enough and we let $r(x) = \sum_{k=j}^{k} r_j(x)$, then
$$
\int |q(x) - r(x)| \, d\mu = \sum_{j=1}^{k} \delta_j c_j < \epsilon.
$$ 

**Proof**

Let $g$ be an integrable function on $\mathbb{R}$ and $\epsilon > 0$.

(i) By the definition of the Lebesgue integral, there exists a simple function $\varphi = \sum_{k=1}^{n} b_k \mathbf{1}_{A_k}$, where $A_k \in \mathcal{L}$ (the Lebesgue sigma-algebra), such that

$$
\int |g - \varphi| \, dx < \epsilon.
$$

(ii) For each $A_k$, by Exercise A.2.1, there exists a finite union of intervals $I_k$ such that $\mu(A_k \triangle I_k) < \delta$, where $\delta > 0$ is chosen small enough so that the total error remains less than $\epsilon$. Define the step function

$$
q = \sum_{j=1}^{m} c_j \mathbf{1}_{(a_{j-1}, a_j)},
$$

where the intervals $(a_{j-1}, a_j)$ partition the support of $\varphi$, and the constants $c_j$ approximate the corresponding $b_k$. Then,

$$
\int | \varphi - q | \, dx < \epsilon.
$$

(iii) To construct a continuous function $r$, replace each term $c_j \mathbf{1}_{(a_{j-1}, a_j)}$ in $q$ with a function $r_j$ defined as follows:

$$
r_j(x) = \begin{cases}
0, & x \leq a_{j-1}, \\
\frac{c_j}{\delta_j} (x - a_{j-1}), & a_{j-1} < x \leq a_{j-1} + \delta_j, \\
c_j, & a_{j-1} + \delta_j < x < a_j - \delta_j, \\
\frac{c_j}{\delta_j} (a_j - x), & a_j - \delta_j \leq x < a_j, \\
0, & x \geq a_j,
\end{cases}
$$

where $\delta_j > 0$ are small enough to ensure continuity and that

$$
\int | q(x) - r(x) | \, dx = \sum_{j=1}^{m} \int_{a_{j-1}}^{a_j} | c_j \mathbf{1}_{(a_{j-1}, a_j)}(x) - r_j(x) | \, dx < \epsilon.
$$

Set $r(x) = \sum_{j=1}^{m} r_j(x)$. Then $r$ is continuous and

$$
\int | g(x) - r(x) | \, dx \leq \int | g(x) - \varphi(x) | \, dx + \int | \varphi(x) - q(x) | \, dx + \int | q(x) - r(x) | \, dx < 3\epsilon.
$$

By adjusting $\epsilon$, we can achieve the desired approximation.

{{< pagebreak >}}

## 1.4.4

**1.4.4.** Prove the Riemann-Lebesgue lemma. If $g$ is integrable then
$$
\lim_{n \to \infty} \int g(x) \cos(nx) \, dx = 0.
$$
Hint: If $g$ is a step function, this is easy. Now use the previous exercise.

**Proof**

Let $g$ be an integrable function on $\mathbb{R}$. We aim to show that

$$
\lim_{n \to \infty} \int_{-\infty}^{\infty} g(x) \cos(nx) \, dx = 0.
$$

First, suppose $g$ is a step function, i.e., $g = \sum_{k=1}^{m} c_k \mathbf{1}_{(a_{k-1}, a_k)}$ with $-\infty < a_0 < a_1 < \dots < a_m < \infty$. Then,

$$
\int_{-\infty}^{\infty} g(x) \cos(nx) \, dx = \sum_{k=1}^{m} c_k \int_{a_{k-1}}^{a_k} \cos(nx) \, dx = \sum_{k=1}^{m} c_k \left[ \dfrac{\sin(nx)}{n} \right]_{a_{k-1}}^{a_k} = \dfrac{1}{n} \sum_{k=1}^{m} c_k \left( \sin(na_k) - \sin(na_{k-1}) \right),
$$

which tends to zero as $n \to \infty$ because $\sin(na_k)$ is bounded and $n$ is in the denominator.

For a general integrable function $g$, by Exercise 1.4.3, for any $\epsilon > 0$, there exists a continuous function $r$ such that $\int | g(x) - r(x) | \, dx < \epsilon$. Since $r$ is continuous and of compact support (due to approximation by step functions with finite intervals), $r$ is uniformly continuous and bounded. The Fourier transform of $r$ vanishes at infinity, so

$$
\lim_{n \to \infty} \int_{-\infty}^{\infty} r(x) \cos(nx) \, dx = 0.
$$

Then,

Here's the reformatted version of the expression over two lines for better readability:

$$
\left| \int g(x) \cos(nx) \, dx \right| \leq \left| \int (g(x) - r(x)) \cos(nx) \, dx \right| 
+ \left| \int r(x) \cos(nx) \, dx \right|
$$

$$
\leq \int | g(x) - r(x) | \, dx + \left| \int r(x) \cos(nx) \, dx \right|.
$$

This way, it looks cleaner, breaking the expression across two lines while maintaining clarity.

By choosing $\epsilon$ small enough, the first term on the right-hand side is less than $\epsilon$, and the second term tends to zero as $n \to \infty$. Therefore,

$$
\limsup_{n \to \infty} \left| \int g(x) \cos(nx) \, dx \right| \leq \epsilon.
$$

Since $\epsilon > 0$ is arbitrary, it follows that

$$
\lim_{n \to \infty} \int g(x) \cos(nx) \, dx = 0.
$$

{{< pagebreak >}}

# Chapter 1.5 Exercises

## **Hints for Exercises**

### **Exercise 1.5.1**
- **Hint**: This exercise asks you to prove an inequality involving integrals and $L^\infty$ norms. Specifically, you're asked to show that:
  $$
  \int |fg| \, d\mu \leq \|f\|_1 \|g\|_\infty
  $$
- **Refer to**: Theorem 1.5.2 (Hölder's Inequality, Page 25). Use the fact that $f \in L^1$ and $g \in L^\infty$, where you can apply the inequalities involving these norms.

### **Exercise 1.5.2**
- **Hint**: You're asked to show that for a probability measure $\mu$, the $L^\infty$ norm can be derived as the limit of $L^p$ norms as $p \to \infty$.
- **Refer to**: Theorem 1.5.2 (Page 25) for inequalities involving $p$-norms and their relationship to the supremum norm $L^\infty$. This exercise is related to the idea that, as $p$ grows larger, $L^p$ norms approach the essential supremum.

### **Exercise 1.5.3**
- **Hint**: Prove **Minkowski's inequality**. This inequality is a fundamental result in $L^p$-spaces and generalizes the triangle inequality.
  - **(i)**: Use Hölder’s inequality (Theorem 1.5.2) and powers of $p$-norms to show the inequality.
  - **(ii)**: Special case when $p = 1$ or $p = \infty$, the calculations simplify, and you should refer to how Hölder's inequality reduces in these cases.
- **Refer to**: Theorem 1.5.2 (Hölder's Inequality, Page 25). This inequality is essential for manipulating $L^p$-norms. Specifically, apply Hölder’s inequality for the proof.

### **Exercise 1.5.4**
- **Hint**: Show that for disjoint sets $E_m$ and a function $f$, summing the integrals over each set equals integrating $f$ over the union $E$.
- **Refer to**: Page 26, Example 1.5.4, which shows how to apply results when sets are disjoint. You can relate this to finite additivity of integrals over disjoint sets.

### **Exercise 1.5.5**
- **Hint**: Given the functions $g_n$, where their integrals are bounded, you're asked to show the behavior of their sums and convergence properties. 
- **Refer to**: The **Monotone Convergence Theorem** (Theorem 1.5.7, Page 27) and **Fatou’s Lemma** (Theorem 1.5.5, Page 26) will be relevant for analyzing the behavior of sums and limits of integrals.

### **Exercise 1.5.6**
- **Hint**: You need to prove that summing the integrals of non-negative functions $g_m$ results in the integral of their sum.
- **Refer to**: The **Monotone Convergence Theorem** (Theorem 1.5.7, Page 27). This is directly related to summing the integrals and taking limits. Also, see Example 1.5.6 for specific cases.

### **Exercise 1.5.7**
- **Hint**: 
  - **(i)**: Use the fact that $f \wedge n$ converges pointwise to $f$, and apply the **Monotone Convergence Theorem**.
  - **(ii)**: Use integrability and the properties of convergence along with Fatou's Lemma.
- **Refer to**: Page 27, Theorem 1.5.7 (Monotone Convergence Theorem) and Fatou’s Lemma (Theorem 1.5.5, Page 26) for guidance on limits of integrals.

### **Exercise 1.5.8**
- **Hint**: Show that for an integrable function on an interval $[a, b]$, the function $g(x) = \int_a^x f(y) \, dy$ is continuous. This is a fundamental property of integrals.
- **Refer to**: The properties of continuous functions and how integrals behave with respect to bounds of integration. Also refer to Theorem 1.4.7 (Page 23), which gives general properties of integrals.

### **Exercise 1.5.9**
- **Hint**: Show that if a function $f$ has a finite $L^p$-norm, then it can be approximated by simple functions $\varphi_n$.
- **Refer to**: Lemma 1.4.4 (Page 22), which discusses the approximation of functions by simpler forms. The idea is to approximate general functions by step functions or simple functions for which the norms converge.

### **Exercise 1.5.10**
- **Hint**: This exercise deals with sums of integrals and interchange of summation and integration. You're asked to show that summing integrals is the same as integrating the sum.
- **Refer to**: Dominated Convergence Theorem (Theorem 1.5.8, Page 27), which allows you to exchange limits and integrals under certain conditions. The same logic applies for summing integrals and integrals of sums.


{{< pagebreak >}}


## 1.5.1

**1.5.1.**  Let $\|f\|_{\infty} = \inf\{M : \mu(\{x : |f(x)| > M\}) = 0\}$. Prove that
$$
\int |fg| d\mu \leq \|f\|_1 \|g\|_{\infty}.
$$

**Proof**

Let $\|f\|_{\infty} = \inf\{ M : \mu( \{ x : |f(x)| > M \} ) = 0 \}$ and $\|g\|_{\infty}$ defined similarly. Then, almost everywhere, $|g(x)| \leq \|g\|_{\infty}$. Therefore,

$$
\int |f g| \, d\mu = \int |f| \, |g| \, d\mu \leq \|g\|_{\infty} \int |f| \, d\mu = \|g\|_{\infty} \|f\|_1.
$$

{{< pagebreak >}}

## 1.5.2

**1.5.2.** Show that if $\mu$ is a probability measure then
$$
\|f\|_{\infty} = \lim_{p \to \infty} \|f\|_p.
$$

**Proof**

Since $\mu$ is a probability measure, $\mu(\Omega) = 1$. For each $p \geq 1$, we have

$$
\|f\|_p = \left( \int |f|^p \, d\mu \right)^{1/p}.
$$

Because $|f| \leq \|f\|_{\infty}$ almost everywhere, it follows that $\|f\|_p \leq \|f\|_{\infty}$. For any $\epsilon > 0$, the set $A_{\epsilon} = \{ x : |f(x)| > \|f\|_{\infty} - \epsilon \}$ has positive measure. Then,

$$
\|f\|_p \geq \left( (\|f\|_{\infty} - \epsilon)^p \mu(A_{\epsilon}) \right)^{1/p} = (\|f\|_{\infty} - \epsilon) \mu(A_{\epsilon})^{1/p}.
$$

As $p \to \infty$, $\mu(A_{\epsilon})^{1/p} \to 1$, so

$$
\lim_{p \to \infty} \|f\|_p \geq \|f\|_{\infty} - \epsilon.
$$

Since $\epsilon > 0$ is arbitrary, we conclude $\lim_{p \to \infty} \|f\|_p = \|f\|_{\infty}$.

{{< pagebreak >}}

## 1.5.3

**1.5.3.** Minkowski’s inequality. (i) Suppose $p \in (1, \infty)$. The inequality $\|f + g\|_p \leq 2^p (\|f\|_p^p + \|g\|_p^p)$ shows that if $\|f\|_p$ and $\|g\|_p$ are $< \infty$ then $\|f + g\|_p < \infty$. Apply Hölder’s inequality to $|f|^{p-1}$ and $|g||f + g|^{p-1}$ to show $\|f + g\|_p \leq \|f\|_p + \|g\|_p$. (ii) Show that the last result remains true when $p = 1$ or $p = \infty$.

**Proof**

(i) For $p \in (1, \infty)$, Minkowski's inequality states:

$$
\|f + g\|_p \leq \|f\|_p + \|g\|_p.
$$

This holds because the function $t \mapsto t^p$ is convex on $[0, \infty)$, and thus $|f + g|^p \leq (|f| + |g|)^p$. Integrating both sides and taking the $p$-th root yields the inequality.

(ii) When $p = 1$, the inequality reduces to:

$$
\int |f + g| \, d\mu \leq \int |f| \, d\mu + \int |g| \, d\mu.
$$

For $p = \infty$, we have:

$$
\|f + g\|_{\infty} = \operatorname{ess\,sup} |f + g| \leq \operatorname{ess\,sup} (|f| + |g|) \leq \|f\|_{\infty} + \|g\|_{\infty}.
$$

Thus, the inequality holds for $p = 1$ and $p = \infty$.

{{< pagebreak >}}

## 1.5.4

**1.5.4.**  If $f$ is integrable and $E_m$ are disjoint sets with union $E$ then
$$
\sum_{m=0}^{\infty} \int_{E_m} f d\mu = \int_{E} f d\mu.
$$
So if $f \geq 0$, then $\nu(E) = \int_{E} f d\mu$ defines a measure.

**Proof**

Since $f$ is integrable and the sets $E_m$ are disjoint with $E = \bigcup_{m} E_m$, we have:

$$
\int_{E} f \, d\mu = \int \left( \sum_{m} \mathbf{1}_{E_m}(x) \right) f(x) \, d\mu(x) = \sum_{m} \int_{E_m} f \, d\mu.
$$

If $f \geq 0$, defining $\nu(E) = \int_{E} f \, d\mu$ gives a measure since $\nu$ is non-negative, countably additive, and $\nu(\emptyset) = 0$.

{{< pagebreak >}}

## 1.5.5

**1.5.5.**  If $g_n \uparrow g$ and $\int g_1^{-} d\mu < \infty$ then $\int g_n d\mu \uparrow \int g d\mu$.

**Proof**

Given $g_n \uparrow g$ and $\int g_1^{-} \, d\mu < \infty$, where $g_1^{-} = \max\{ -g_1, 0 \}$, we have $g_n^{-} \leq g_1^{-}$ for all $n$. Therefore, $\int g_n^{-} \, d\mu \leq \int g_1^{-} \, d\mu < \infty$. Since $g_n^{+} = \max\{ g_n, 0 \}$ increases to $g^{+}$, by the Monotone Convergence Theorem,

$$
\int g_n \, d\mu = \int g_n^{+} \, d\mu - \int g_n^{-} \, d\mu \uparrow \int g^{+} \, d\mu - \int g^{-} \, d\mu = \int g \, d\mu.
$$

{{< pagebreak >}}

## 1.5.6

**1.5.6.**  If $g_m \geq 0$ then $\int \sum_{m=0}^{\infty} g_m d\mu = \sum_{m=0}^{\infty} \int g_m d\mu$.

**Proof**

For non-negative functions $g_m$, the function $G = \sum_{m=0}^{\infty} g_m$ is measurable and takes values in $[0, \infty]$. By the Monotone Convergence Theorem,

$$
\int \sum_{m=0}^{\infty} g_m \, d\mu = \lim_{N \to \infty} \int \sum_{m=0}^{N} g_m \, d\mu = \lim_{N \to \infty} \sum_{m=0}^{N} \int g_m \, d\mu = \sum_{m=0}^{\infty} \int g_m \, d\mu.
$$

{{< pagebreak >}}

## 1.5.7

**1.5.7.**  Let $f \geq 0$. (i) Show that $\int f \land n d\mu \uparrow \int f d\mu$ as $n \to \infty$. (ii) Use (i) to conclude that if $g$ is integrable and $\epsilon > 0$ then we can pick $\delta > 0$ so that $\mu(A) < \delta$ implies $\int_A |g| d\mu < \epsilon$.

**Proof**

(i) Since $f \geq 0$ and $f \wedge n \leq f \wedge (n+1) \leq f$, the sequence $f \wedge n$ increases to $f$ pointwise. By the Monotone Convergence Theorem,

$$
\lim_{n \to \infty} \int f \wedge n \, d\mu = \int f \, d\mu.
$$

(ii) Given $\epsilon > 0$ and $g$ integrable, there exists $N$ such that

$$
\int_{ \{ |g| > N \} } |g| \, d\mu < \epsilon.
$$

Let $\delta = \epsilon / N$. Then, for any measurable set $A$ with $\mu(A) < \delta$,

$$
\int_A |g| \, d\mu = \int_{A \cap \{ |g| \leq N \} } |g| \, d\mu + \int_{A \cap \{ |g| > N \} } |g| \, d\mu \leq N \mu(A) + \epsilon < \epsilon + \epsilon = 2\epsilon.
$$

Adjusting $\epsilon$ yields the desired result.

{{< pagebreak >}}

## 1.5.8

**1.5.8.** Show that if $f$ is integrable on $[a, b]$, $g(x) = \int_{[a,x]} f(y) dy$ is continuous on $(a,b)$.

**Proof**

Since $f$ is integrable on $[a, b]$, for any $x, y \in [a, b]$,

$$
|g(x) - g(y)| = \left| \int_{[a, x]} f(t) \, dt - \int_{[a, y]} f(t) \, dt \right| = \left| \int_{[x, y]} f(t) \, dt \right| \leq \int_{[x, y]} |f(t)| \, dt.
$$

As $x \to y$, the right-hand side approaches zero because $f$ is integrable. Therefore, $g$ is continuous on $[a, b]$.

{{< pagebreak >}}

## 1.5.9

**1.5.9.** Show that if $f$ has $\|f\|_p = (\int |f|^p d\mu)^{1/p} < \infty$, then there are simple functions $\varphi_n$ so that $\|\varphi_n - f\|_p \to 0$.

**Proof**

Given $f \in L^p$, there exists a sequence of simple functions $\varphi_n$ such that $\| f - \varphi_n \|_p \to 0$. This follows from the density of simple functions in $L^p$ spaces. One can construct $\varphi_n$ by truncating $f$ and approximating the truncated function by simple functions.

{{< pagebreak >}}

**1.5.10.** Show that if $\sum_n \int |f_n| d\mu < \infty$ then $\int \sum_n f_n d\mu = \sum_n \int f_n d\mu$.

**Proof**

Since $\sum_n \int |f_n| \, d\mu < \infty$, the series $\sum_n f_n(x)$ converges absolutely for almost every $x$. Define $S(x) = \sum_n f_n(x)$. Then,

$$
\int S \, d\mu = \int \sum_n f_n \, d\mu = \sum_n \int f_n \, d\mu,
$$

where the interchange of summation and integration is justified by Fubini's theorem due to the absolute convergence of the integrals of $|f_n|$.

{{< pagebreak >}}